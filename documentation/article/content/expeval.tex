\section{Evaluation}\label{sect:eval}
In the following section, we evaluate \FW{}. The aim of the evaluation is to test that the framework lets testers write less code, and still execute tests at a reasonable speed. A reasonable execution speed should not differ much from speeds of other test methods.   In the following, we describe our criteria and how an experiment was performed. Finally we present our results and evaluate. 

\subsection{Criteria of Evaluation}\label{subsect:CoE}
As mentioned in \cref{intro}, no test framework on market is developed especially for pygrametl. So there is no obvious candidate to compare against. Instead we may evaluate against more generic on-market products, such as QuerySurge and AnyDBTest. However because of time constraints, we opt not to use these tools, as they need to be learned. Instead we choose to evaluate against manual testing, which tests code through SQL queries.  While this may not be the fairest of comparisons, the evaluation will show the potential gain of migrating to \FW{} from manual testing. 

Now that we have determined what to evaluate \FW{} against, we describe the criteria used for the evaluation.  

\begin{description}
\item[Number of statements used] The best solution uses the fewest statements to test an ETL system. In general, as the amount of statements increase, a program becomes more difficult to comprehend. As the size grows, more time will have to be spend on general development and debugging.
\item[Execution time] The best solution spends the shortest amount of time on executing tests. Any test involving databases will often involve large amounts of data. An efficient runtime mean the difference between a test running for a few hours or overnight. 
\end{description}

Note that we evaluate based mostly upon the number of statements used. We accept an increase in runtime, if the program becomes easier to comprehend.

Another possible criteria could have been implementation time. However, we have already worked in depth with SQL queries internal to the framework predicates. These queries are very similar to the ones needed for manual testing. Thus we expect implementation time for manual testing to be rather fast, as we are already familiar with these queries.  As this may not be representative of the time needed for manual testing, we decide against using this criteria.  

\subsection{Experiment}
Having now defined our criteria, we perform an experiment, to gather data for the evaluation of \FW{} against manual testing. 

For the experiment we created a small pygrametl program to populate the BookAndAuthor DW as presented in \cref{sect:Overview}. The program is made to be erroneous. For sources, the program uses two SQL databases, containing data on books and authors, and a single csv-file storing pairs of city and country names.  Once the sources have been set up, an empty DW is populated using the pygrametl program. 

Once the DW is filled, we perform tests using both  \FW{} and manual.  Manual testing was done using SQLite, through the sqlite3 python module. In both instances, testing  cover the following test cases. They are chosen, as to use every predicate that FW{} offers.

\begin{itemize}
\item No nulls appear in any column of authorDim
\item There is functional dependency between cid and city in the join between authorDim and bookDim. 
\item Referential integrity is upheld through the entire DW
\item All (name,cid) pairs within authorDim are unique.
\item BookDim contains 6 rows
\item In bookDim, the highest version value of the book “EZ PZ ETL” is 4.
\item Content goodbooksdim is a subset of bookDim. Goodbooksdim contains two rows for the books “The Hobbit” and “The Divine Comedy”. The comparison should not take account of duplicates.
[\end{itemize}

During experimentation we strive to make sure that both tests deliver the same results.  This is important as \FW{} focuses on reporting faulty data to the tester. This does slow down performance, compared to if we merely reported whether a predicate passed or failed. To make the two methods comparable, manual testing must live up to the same requirement of verbosity when reporting.

\subsection{Results}
After testing with both methods, we measured the number of statements and execution time of each. When counting statements for manual testing, we counted each clause in an SQL query as a separate statement. If a where-clause contains more than a single logical clause, all additional logical clauses are counted as statements themselves.  Tests were performed, where authorDim and bookDim had 105 rows, countryDim 2 and factTable 107. The results of the measurements can be seen in \cref{table:result}.

It is obvious that \FW{} allows testers to use less statements in their tests. This strongly indicates that more expressive power is contained within each statement. Looking at the execution time, it seems that manual testing performs much faster, running in 0.015 seconds as compared to 3.50 seconds, when using the framework. However, most of those 3.5 seconds are spent on running a DWPopulator object to get ready for testing. As explained in \cref{sec:dwpopulator}, this entails the running of the ETL program, and creating the DWRepresentation object used by the predicates. Not accounting for this process, the actual tests execute in 0.015 seconds, just like with manual. It must also be remembered that the time to run the The DWPopulator does not increase with the size of the DW. Only the runtime of the  testing portion depends on the amount of data stored. Thus this part of the execution will not matter, when testing upon larger amounts of data.


\begin{table}[h]
\centering
\caption{Result of Evaluation}
\label{table:result}
\begin{tabular}{|p{0.10\textwidth}|p{0.15\textwidth}|p{0.15\textwidth}|}
\hline
												& \FW{}	 	& Manual				 \\ \hline

Number of statements  & 14 & 111 \\ \hline
Execution Time	& 3.5 s   & 0.015 s   \\ \hline
\end{tabular}
\end{table}







