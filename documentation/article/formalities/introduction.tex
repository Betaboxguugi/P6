\section*{Introduction} % The \section*{} command stops section numbering
\addcontentsline{toc}{section}{Introduction} % Adds this section to the table of contents

To make sure that a piece of software can be expected to run as intended, it must be tested. Today we have specialized software that assist in automated and regression testing of systems. This allows testers to focus more on what to test and less on test implementation. Conceptually this should lead to tests of a higher quality.

There exists a large amount of test software that assist in the general aspects of testing. Developers may however experience, these tools to not be the best fit for their systems. Sometimes there is a need for test software to conform to one particular type of system. For this article we will focus on the testing of Extract-Transform-Load (ETL) systems. These systems support the creation and update of an On-Line Analytical Processing (OLAP) data warehouse (DW). ETL assists in extracting data from a set of sources. Then it applies some transformation upon the data, before this is loaded into the DW. The purpose of the DW is to do business analysis, and the ETL manages the dataflow to the DW. Often GUI-based tools are used, when developing these ETL systems. The pygrametl python package is open source, and allows for coding ETL systems instead. The idea being that experts perform better when using an API rather than a GUI \cite{thomsen2009pygrametl}. 

Systems for automatic ETL testing such as QuerySurge and DBFit are already on market. However, as related by principal business analytics architect of element61, Davy Knuysen: “Tools dedicated to ETL testing exist, but the mature ones are still rare”\cite{element61}. Knuysen especially focuses on the lack of quality open source solutions. There is even some suggestion developers set up their own automation. This suggests the need for further development, especially on the open source front. At the same time we see that no dedicated ETL testing tool is currently available for pygrametl.  We wish to  develop an automatic test framework for this package. The goal being not only to benefit the users of the package, but also expand on ideas from existing products. Thus we aim to progress  the field of ETL testing.  

Users of pygrametl are deemed to be database experts with a lot of coding experience. Thus writing unit tests for a pygrametl program is not a major issue for them. Though, doing integration testing of the system seems more difficult. When doing a test of the  ETL program as a whole, a tester may view the system as a black box. This box takes as input some data sources along with an established DW. It then outputs an updated DW after load. This DW is then compared to our expectations of what the box should output. The test succeeds or fails based on this comparison. Our framework should support this type of testing. There should also be support for testing of  individual extract, transform and load components. The aim being that the framework can be used for regression testing. The reason for this is that ETL systems usually evolve over time due to changes in its data sources. And that many of the different processes can be interlinked, so that changes in one may affect others.

When developing this framework, there are two major issues that we need to contend with. How should input for test cases be specified? And how should the test be run?  Input to our test cases is made up of two components, the input to the process and the expected output. We want our tests to be fast to implement. Thus we need to find an input format that allows testers to quickly specify input databases. This task may be too cumbersome if done with SQL. There is also the question, of how to specify expected output. In some instances, it may be appropriate to set up an entire DW as an expected output. Yet, what if we only care about certain details such as the schemas of tables? The issue of how tests should be run is one of performance. Not only should the tests be fast to implement, but also fast to execute. So we must watch out for some time complexity pitfalls. For instance, it is costly to brute force the comparasion of the actual output DW to the one we expected. We have to find effective ways of performing tasks like these during tests. 

%Terra incognita
%Here be related works.

%Doot doot
%Here be contents
